---
phase: 23-foundation-infrastructure
plan: 05
type: execute
wave: 2
depends_on: ["23-01"]
files_modified:
  - backend/public/robots.txt
  - backend/index.js
autonomous: true

must_haves:
  truths:
    - "robots.txt is served at the site root with correct content type"
    - "Admin paths are blocked from all crawlers"
    - "API routes are blocked from all crawlers"
    - "AI training bots are explicitly blocked"
    - "AI search bots are allowed for discoverability"
    - "Frontend assets are served from Express static middleware"
  artifacts:
    - path: "backend/public/robots.txt"
      provides: "Crawler directives file"
      contains: "User-agent: GPTBot"
  key_links:
    - from: "backend/index.js"
      to: "backend/public/"
      via: "express.static for public directory"
      pattern: "express\\.static.*public"
---

<objective>
Create robots.txt with crawler directives and configure Express to serve frontend static assets (Parcel-built JS/CSS) alongside the robots.txt file.

Purpose: robots.txt tells search engines which paths to crawl and blocks AI training bots from scraping original creative content. Static asset serving prepares for the deployment merge where Express serves everything (not just API). These are foundational for all SSR pages.
Output: robots.txt at site root, Express static middleware for public/ directory and frontend dist/.
</objective>

<execution_context>
@C:/Users/pagis/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/pagis/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/23-foundation-infrastructure/23-RESEARCH.md
@backend/index.js
@backend/package.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create robots.txt with crawler directives</name>
  <files>backend/public/robots.txt</files>
  <action>
  Create `backend/public/robots.txt` with the following content:

  ```
  # Tamar Kfir Jewelry — Crawler Directives

  # Default rules for all crawlers
  User-agent: *
  Disallow: /admin/
  Disallow: /api/
  Allow: /

  # Block AI training crawlers (original creative content protection)
  User-agent: GPTBot
  Disallow: /

  User-agent: ClaudeBot
  Disallow: /

  User-agent: CCBot
  Disallow: /

  User-agent: Google-Extended
  Disallow: /

  User-agent: Applebot-Extended
  Disallow: /

  User-agent: PerplexityBot
  Disallow: /

  User-agent: Claude-Web
  Disallow: /

  # Allow AI search/assistant crawlers (preserve discoverability)
  User-agent: Claude-SearchBot
  Allow: /

  User-agent: ChatGPT-User
  Allow: /
  ```

  Per locked decisions:
  - Block admin paths (Disallow /admin/)
  - Block API routes (Disallow /api/)
  - Block AI training bots (GPTBot, CCBot, Google-Extended, and similar)

  Per research recommendations:
  - Allow AI search bots (Claude-SearchBot, ChatGPT-User) for discoverability
  - Omit sitemap reference for now (Phase 25 will add it when sitemap.xml exists)

  The backend/public/ directory already exists (it has an images/ subdirectory). Place robots.txt directly inside it.
  </action>
  <verify>`cat backend/public/robots.txt | head -5` — should show the comment and User-agent lines. Verify file is not empty.</verify>
  <done>robots.txt exists at backend/public/robots.txt with admin/API blocking, AI training bot blocking, and AI search bot allowing.</done>
</task>

<task type="auto">
  <name>Task 2: Configure Express static middleware for public assets and frontend dist</name>
  <files>backend/index.js</files>
  <action>
  In backend/index.js, add static middleware for the public directory and frontend dist. The backend already has `express.static` calls for image directories. Add these NEW static middleware entries:

  1. Serve robots.txt and other public files from backend/public/:
     ```javascript
     // Serve public assets (robots.txt, etc.)
     app.use(express.static(path.join(__dirname, 'public'), {
       maxAge: '1d',
       index: false  // Don't serve index.html from public/
     }));
     ```
     Place this BEFORE the SSR routes but AFTER the existing image static middleware. The `index: false` prevents Express from serving an index.html file from public/ which could conflict with the root redirect.

  2. Serve frontend Parcel-built assets (INFRA-05):
     ```javascript
     // Serve frontend dist (Parcel-built JS/CSS bundles)
     app.use('/dist', express.static(path.join(__dirname, '..', 'frontend', 'dist'), {
       maxAge: '7d',
       immutable: true  // Parcel uses content hashes in filenames
     }));
     ```
     This makes frontend bundles available at /dist/js/*, /dist/css/* paths.

  3. Serve frontend static images:
     ```javascript
     // Serve frontend images
     app.use('/imgs', express.static(path.join(__dirname, '..', 'frontend', 'imgs'), {
       maxAge: '7d'
     }));
     ```

  4. Serve favicon:
     ```javascript
     app.use('/favicon.ico', express.static(path.join(__dirname, '..', 'frontend', 'favicon.ico')));
     ```

  IMPORTANT: Place all these static middleware calls BEFORE the trailingSlashRedirect and route handlers to ensure static files are served efficiently without going through the routing pipeline. Place them in a clearly commented "Static Assets" section.

  Do NOT remove any existing static middleware — only add new entries.
  </action>
  <verify>
  Start the dev server. Test:
  - `curl -s -o /dev/null -w "%{http_code}" http://localhost:PORT/robots.txt` — should return 200
  - `curl -s http://localhost:PORT/robots.txt` — should contain "User-agent: GPTBot"
  - `curl -s -o /dev/null -w "%{http_code}" http://localhost:PORT/dist/` — verifies the static path is registered (may 404 if no dist files, but should not error)
  - Existing tests pass: `cd backend && npm test`
  </verify>
  <done>Express serves robots.txt from /robots.txt, frontend dist assets from /dist/*, frontend images from /imgs/*, and favicon.ico. All existing routes and tests continue to work.</done>
</task>

</tasks>

<verification>
1. `curl http://localhost:PORT/robots.txt` — returns robots.txt content with correct User-agent rules
2. Content-Type header for robots.txt is text/plain
3. robots.txt contains "Disallow: /admin/" and "Disallow: /api/"
4. robots.txt contains "User-agent: GPTBot" with "Disallow: /"
5. robots.txt contains "User-agent: Claude-SearchBot" with "Allow: /"
6. Static middleware registered for /dist/ and /imgs/ paths
7. `cd backend && npm test` — all tests pass
</verification>

<success_criteria>
- robots.txt served at /robots.txt with text/plain content type
- Admin paths and API routes blocked for all crawlers
- 7 AI training bots explicitly blocked (GPTBot, ClaudeBot, CCBot, Google-Extended, Applebot-Extended, PerplexityBot, Claude-Web)
- 2 AI search bots explicitly allowed (Claude-SearchBot, ChatGPT-User)
- Frontend dist/, imgs/, and favicon.ico served via Express static
- No sitemap reference yet (deferred to Phase 25)
- All 419 existing tests continue to pass
</success_criteria>

<output>
After completion, create `.planning/phases/23-foundation-infrastructure/23-05-SUMMARY.md`
</output>
